{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\user\\Desktop\\Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'.\\mix_platforms.csv', encoding='utf-8',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['platforms','pledged_percent','pledged_usd'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('project_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.get_dummies(data,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_v1.drop(\"status\", axis=1).values\n",
    "y = data_v1[\"status\"].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC & AUC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate_model(predicted_prob, predicted, actual):\n",
    "\n",
    "    def Roc_Auc(predicted_prob, actual):\n",
    "        fpr, tpr, thresholds = roc_curve(actual, predicted_prob[:,1], pos_label=None)\n",
    "        return auc(fpr, tpr)\n",
    "    \n",
    "    # Table-Type Plotting\n",
    "    #print('Confusion Matrix:\\n{}'.format(confusion_matrix(predicted, actual)))\n",
    "    print('Classification Report:\\n{}'.format(classification_report(predicted, actual)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(predicted, actual)))\n",
    "    print('Precision: {}'.format(precision_score(predicted, actual)))\n",
    "    print('Recall: {}'.format(recall_score(predicted, actual)))\n",
    "    print('F-1: {}'.format(f1_score(predicted, actual)))\n",
    "    print('AUC: {}'.format(Roc_Auc(predicted_prob, actual)))  \n",
    "        \n",
    "    # ROC Curve Plotting\n",
    "    fpr, tpr, thresh = roc_curve(actual, predicted_prob[:,1], pos_label=None)\n",
    "    roc_auc = Roc_Auc(predicted_prob, actual)   \n",
    "    plt.title('ROC')\n",
    "    plt.plot(fpr, tpr, 'b',\n",
    "    label='AUC = %0.2f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.2])\n",
    "    plt.ylim([-0.1,1.2])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Confusion Matrix:\\n{}'.format(confusion_matrix(predicted, actual)))\n",
    "    tp, fp, fn, tn  = confusion_matrix(actual, predicted).ravel()\n",
    "    print(\"True positives: \" + str(tp))\n",
    "    print(\"False positives: \" + str(fp))\n",
    "    print(\"True negatives: \" + str(tn))\n",
    "    print(\"False negatives: \" + str(fn))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "gbm_model = lgb.LGBMClassifier()\n",
    "gbm_model.fit(X_train,y_train, eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=gbm_model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "gbm_AUC=metrics.auc(fpr, tpr)\n",
    "print(gbm_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(gbm_model.predict_proba(X_test), \n",
    "               gbm_model.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgbc_model=xgb.XGBClassifier()\n",
    "xgbc_model.fit(X_train, y_train, eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=xgbc_model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "xgbc_AUC=metrics.auc(fpr, tpr)\n",
    "print(xgbc_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(xgbc_model.predict_proba(X_test), \n",
    "               xgbc_model.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logreg_model = linear_model.LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=logreg_model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "logreg_AUC=metrics.auc(fpr, tpr)\n",
    "print(logreg_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(logreg_model.predict_proba(X_test), \n",
    "               logreg_model.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "svm = LinearSVC()\n",
    "clf = CalibratedClassifierCV(svm) \n",
    "clf.fit(X_train, y_train)\n",
    "y_proba = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "svm_AUC=metrics.auc(fpr, tpr)\n",
    "print(svm_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(svm_model.predict_proba(X_test), \n",
    "               svm_model.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=model_rf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "rf_AUC=metrics.auc(fpr, tpr)\n",
    "print(rf_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_rf.predict_proba(X_test), \n",
    "               model_rf.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_dct = DecisionTreeClassifier()\n",
    "model_dct.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=model_dct.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "dct_AUC=metrics.auc(fpr, tpr)\n",
    "print(dct_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_dct.predict_proba(X_test), \n",
    "               model_dct.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adab_model = AdaBoostClassifier()\n",
    "adab_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=adab_model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "adab_AUC=metrics.auc(fpr, tpr)\n",
    "print(adab_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(adab_model.predict_proba(X_test), \n",
    "               adab_model.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_model = MLPClassifier()\n",
    "mlp_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "testy_pred_prob=mlp_model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "mlp_AUC=metrics.auc(fpr, tpr)\n",
    "print(mlp_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mlp_model.predict_proba(X_test), \n",
    "               mlp_model.predict(X_test), \n",
    "               y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking:\n",
    "    \n",
    "    def __init__(self, x_train, y_train, x_test, y_test, ls_model_stack, model_final, type_label):\n",
    "        self.trainx = x_train\n",
    "        self.trainy = y_train\n",
    "        self.testx = x_test\n",
    "        self.testy = y_test\n",
    "        self.ls_model_stack = ls_model_stack\n",
    "        self.model_final = model_final\n",
    "        self.type_label = type_label\n",
    "        self.trainx_stacking_input = None\n",
    "        self.trainy_stacking = None\n",
    "        self.trainx_final = None\n",
    "        self.trainy_final = None\n",
    "        self.trainx_final_input = None\n",
    "        self.testx_final_input = np.zeros((x_test.shape[0], len(ls_model_stack)))\n",
    "    \n",
    "    def spliting(self):\n",
    "        if self.type_label == 'discrete':\n",
    "            self.trainx_stacking_input, self.trainx_final, self.trainy_stacking, self.trainy_final = train_test_split (self.trainx, self.trainy, test_size= 0.3, random_state = 42, stratify = self.trainy)\n",
    "        elif self.type_label == 'continuous':\n",
    "            self.trainx_stacking_input, self.trainx_final, self.trainy_stacking, self.trainy_final = train_test_split (self.trainx, self.trainy, test_size= 0.3, random_state = 42)\n",
    "        self.trainx_final_input = np.zeros((self.trainx_final.shape[0], len(self.ls_model_stack)))   \n",
    "    \n",
    "    def modeling_stack_training(self):\n",
    "        for model, i in zip(self.ls_model_stack, range(len(self.ls_model_stack))):\n",
    "            model.fit(self.trainx_stacking_input, self.trainy_stacking)\n",
    "            output = model.predict(self.trainx_final)\n",
    "            self.trainx_final_input[:, i] = output\n",
    "            self.save_model(model, str(i))\n",
    "            \n",
    "    def modeling_final_training(self):\n",
    "        self.model_final.fit(self.trainx_final_input, self.trainy_final)\n",
    "        self.save_model(self.model_final, 'final')\n",
    "        \n",
    "    def predict_stack_testing(self):\n",
    "        for model, i in zip(self.ls_model_stack, range(len(self.ls_model_stack))):\n",
    "            output = model.predict(self.testx)\n",
    "            self.testx_final_input[:, i] = output\n",
    "    \n",
    "    def predict_final_testing(self):\n",
    "        self.testy_pred = self.model_final.predict(self.testx_final_input)\n",
    "        self.testy_pred_prob=self.model_final.predict_proba(self.testx_final_input)\n",
    "        return self.testy_pred, self.testy_pred_prob\n",
    "    \n",
    "    def scoring_testing(self):\n",
    "        if self.type_label == 'discrete':\n",
    "            fpr, tpr, thresholds = roc_curve(self.testy, self.testy_pred_prob[:, 1], pos_label=None)\n",
    "            AUC=auc(fpr, tpr)\n",
    "            return AUC\n",
    "        elif self.type_label == 'continuous':\n",
    "            r2 = r2_score(self.testy, self.testy_pred)\n",
    "            return r2\n",
    "    \n",
    "    def save_model(self, model, filename):\n",
    "        pickle.dump(model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stack=[LogisticRegression(),\n",
    "             CalibratedClassifierCV(LinearSVC()),\n",
    "             RandomForestClassifier(),\n",
    "             DecisionTreeClassifier(),\n",
    "             AdaBoostClassifier(),\n",
    "             XGBClassifier(),\n",
    "             MLPClassifier()]\n",
    "                           \n",
    "model_final=LogisticRegression()\n",
    "type_label = 'discrete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking = Stacking(X_train, y_train, X_test, y_test, model_stack, model_final, type_label)\n",
    "stacking.spliting()\n",
    "stacking.modeling_stack_training()\n",
    "stacking.modeling_final_training()\n",
    "stacking.predict_stack_testing()\n",
    "stacking.predict_final_testing()\n",
    "stacking.scoring_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy_pred, testy_pred_prob = ensemble.predict_final_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, testy_pred_prob[:,1], pos_label=None)\n",
    "ensemble_AUC=metrics.auc(fpr, tpr)\n",
    "print(ensemble_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(testy_pred_prob,\n",
    "               testy_pred,  \n",
    "               testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = pd.DataFrame({\n",
    "    'Model': [\n",
    "              'LightGBM',\n",
    "              'Logistic Regression', \n",
    "              'Random Forest', \n",
    "              'Decision Tree', \n",
    "              'SVM-Linear SVC', \n",
    "              'AdaBoostClassifier',\n",
    "              'Extreme Gradient Boosting (XGBoost)',\n",
    "              'MLP',\n",
    "              'Ensemble'],\n",
    "    'AUC_test': [gbm_AUC,\n",
    "                logreg_AUC, \n",
    "                rf_AUC, \n",
    "                dct_AUC, \n",
    "                svm_AUC, \n",
    "                adab_AUC, \n",
    "                xgbc_AUC,\n",
    "                mlp_AUC,\n",
    "                ensemble_AUC]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records['AUC']=round(records['AUC'],4).values\n",
    "records.sort_values(by='AUC', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
